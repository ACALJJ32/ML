# 第一部分 分类
## 第2章 K近邻算法  
&emsp;&emsp;k-近邻算法采用测量不同特征值之间的距离的方法进行分类。
### * 2.1 k-近邻算法概述  
&emsp;&emsp;* 算法伪代码
&emsp;&emsp;&emsp;&emsp;对未知类别的属性的数据集中的每个点以此执行以下操作：  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;计算已知类别数据集中的每个点与当前点之间的距离；
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;按照距离递增的次序排序；
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;选取与当前点距离最小的k个点；
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;确定前k个点所在类别的出现频率；
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;返回前k个点出现频率最高的类别作为当前点的预测分类；
&emsp;&emsp;* 欧式距离公式
$$d = \sqrt{(xA_{0} - xB_{0})^{2} + (xA_{1} - xB_{1})^{2} }$$
&emsp;&emsp;* 数据归一化  
$$newValue = (oldValue - min)/(max-min)$$
&emsp;&emsp;可以将所有数字转化到0到1的区间。
### * 2.2 小结
&emsp;&emsp;优点：精度高、对异常值不敏感、无数据输入假定。
&emsp;&emsp;缺点：计算复杂度高、空间复杂度高。


## 第3章 决策树
## 第4章 朴素贝叶斯
## 第5章 Logistic回归
## 第6章 支持向量机
## 第7章 AdaBoost















# 2.回归  
## 第8章 回归
## 第9章 树回归  
&emsp;&emsp;第9章介绍的是一种分类回归树CART(Classification And Regression Trees),该算法可以用于回归和分类。
### * 9.1 复杂数据的局部建模
&emsp;&emsp;和第3章的决策树不同，决策树是一种贪心算法，需要在规定的时间内给出合适的决策，并不能考虑到全局最优。并且决策树使用的算法是ID3，每次选取当前的最佳特征去切割，特征使用后，在之后的划分过程中将不会再起作用，并且ID3不可以处理连续性的数据。CART可以通过二元切割来划分数据，并且稍作修改就可以处理回归问题。
### * 9.2 连续和离散型特征数的构建
&emsp;&emsp;首先构建数据类型，新建一个字典，里面包含如下的数据：待切分的特征；待切分的特征值；右子树(不需要切分时是单值)；左子树。函数createTree()的伪代码如下：  
```
找到最佳的待切分特征：
    如果该节点不能划分，将该节点存为叶节点
    执行二元划分
    在右子树调用createTree()
    在左子树调用createTree()
```  
首先给出了三个函数$loadDataSet()$，$binSplitDataSet()$，$createTree()$,我们主要介绍函数$createTree()$.
```python
def createTree(dataSet,leafType=regLeaf,errType=regErr,ops=(1,4)):
    feat,val = chooseBestSplit(dataSet,leafType,errType,ops)
    if feat == None: return val
    # 创建一个空字典，'spInd'记录的当前的特征，'spVal'记录的是特征值val
    retTree()
    retTree['spInd'] = feat
    retTree['spVal'] = val
    #通过二分切割，得到两棵不同的子树
    lSet,rSet = binSplitDataSet(dataSet,feat,val)
    retTree['left'] = create(lSet,leafType,errType,ops) 
    retTree['right'] = create(rSet,leafType,errType,ops)
    return retTree
```  
### * 9.3 将CART用于算法  
#### * 9.3.1 构建数  
&emsp;&emsp;$为了让createTree()$函数运行，需要实现$chooseBestSplit()$函数，给定某个误差的计算方法，该函数会找到数据集的最佳二元切分方式。$chooseBestSplit()$只需要做两件事：切分数据集、生成相应的叶节点。其中，$leafType$是对创建叶节点的函数的引用，errType是对计算总方差函数的引用，$ops$是用户自定义的参数。  
```python
def chooseBestSplit(dataSet,leafType=regLeaf,errType=regErr,ops=(1,4)):
    # tolS,tolN这两个参数用来控制函数的停止时机。tolS是容许误差的下降值，tolN是切分的最少样本数
    tolS = ops[0]; tolN = ops[1] 
    # 如果数据集中所有的值相等，就推出
    if len(set(dataSet[:,-1].T.tolist()[0])) == 1:
        return None,leafType=(dataSet)  
    m,n = shape(dataSet)
    # S是当前计算得到的总方差
    S = errType(dataSet)
    bestS = inf; bestIndex = 0; bestValue = 0
    for featIndex in range(n-1):
        for splitVal in set(dataSet[:,featIndex]):
            mat0,mat1 = binSplitDataSet(dataSet,featIndex,splitVal)
            # 如果当前的划分不满足条件，就跳过当前步，直接进行下一步
            if(shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN):continue
            if newS < bestS:
                bestIndex = featIndex
                beatValue = splitVal
                bestS = newS
    # 如果误差的减少的不够大，就直接退出
    if (S - bestS) < tolS:
        return None,leafType(dataSet)
    mat0,mat1 = binSplitDataSet(dataSet,bestIndex,bestValue)
    # 如果切分的数据集很小，就直接推出
    if (shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN):
        return None,leafType(dataSet)
    return bestIndex,bestValue
``` 
### * 9.4 树剪枝  
&emsp;&emsp;如果一棵树的节点过多，那么该模型可能会出现‘过拟合’现象，为了应对这种情况，需要对模型进行剪枝(pruning)，有两种剪枝方式，预剪枝(prepruning)和后剪枝(postpruning)，后剪枝需要使用测试集和训练集。  
#### * 9.4.1 预剪枝  
&emsp;&emsp;预剪枝就是在函数中提前结束对模型的划分，但是对参数$ops$较为敏感。  
#### * 9.4.2 后剪枝  
&emsp;&emsp;算法伪代码
&emsp;&emsp;&emsp;&emsp;基于已有的树切分测试数据：
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;若存在任意一个子集是一棵树，则在该子集递归剪枝过程；
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;计算将两个叶子节点合并后的误差；
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;计算不合并的误差；
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;如果合并会降低误差的话，就将叶节点合并；
```python
def prune(tree,testData):
    # 首先判断当前节点是否是叶节点，是则直接返回数值类型数据
    if shape(testData)[0] == 0: return getMean(tree)
    
    # 判断左右两个数是否为子树
    if(isTree(tree['right'])) or (isTree(tree['left'])):
        lSet,rSet = binSplitDataSet(testData,tree['spInd'],tree['spVal'])
    
    if isTree(tree['left']): tree['left'] = prune(tree['left'],lSet)
    if isTree(tree['right']):tree['right'] = prune(tree['right'],rSet)  

    # 最后判断是否将两个子节点合并，根据误差来判断  
    if not isTree(tree['left']) and not isTree(tree['right']):  
        lSet,rSet = binSplitDataSet(testDatad,tree['spInd'],tree['spVal'])  
        errorNoMerge = sum(power(lSet[:,-1] - tree['left'],2) + power(rSet[:,-1] - tree['right']),2)  
        treeMean = (tree['left'] + tree['right']) / 2.0
        errorMerge = sum(power(testData[:,-1] - treeMean,2))
        if errorMerge < errorNoMerge:
            print "merging"
            return treeMean
        else: return tree
    else: return tree
```
### * 9.5 模型树  
&emsp;&emsp;用树来对模型进行建模，除了将叶节点设置为常数值之外，还可以将叶节点设置为分段函数，也就是所为的分段线性函数。  
```python  
# 用于执行简单的线性回归
def linearSolve(dataSet):
    m,n = shape(dataSet)
    X = mat(ones(m,n)); Y = mat(ones((m,1)))
    X[:,1:n] = dataSet[:,0:n-1]; Y = mat(ones((:,-1)))
    xTx = X.T * X
    # 如果矩阵可逆
    if linalg.det(xTx) == 0.0:
        raise NameError('This matrix is singular,cannot do inverse,try increasing the second value of ops')
    ws = xTx.I * (X.T * Y)
    return ws,X,Y

# 当节点不需要切分时，负责生成叶节点
def modelLeaf(dataSet):
    ws,X,Y = linearSolve(dataSet)
    return ws


# 计算误差
def modelErr(dataSet):
    ws,X,Y = linearSolve(dataSet)
    yHat = X * ws
    return sum(power(Y-yHat),2)
```
### * 9.6 示例
&emsp;&emsp;下面给出用树回归进行预测的代码。  
```python
# 要对回归树叶节点进行预测，就要调用regTreeVal()
def regTreeEval(model,inDat):
    return float(model)

# 要对模型树节点进行预测，就要调用modelTreeEval()
def modelTreeEval(model,inDat):
    n = shape(inDat)[1] #返回输入数据的列数
    X = mat(ones((1,n+1)))
    X[:,1:n+1] = inDat
    return float(X*model)

#对于输入的单个数据点或者行向量，函数createForeCast()会返回一个浮点值，对于单个数据点，函数会返回一个预测值  
def treeForeCast(tree,inData,model=regTreeEval):
    # 如果输入的是一个数据，不是一棵树，就返回值
    if not isTree(tree): return modelEval(tree,inData)
    if inData[tree['spInd']] > tree['inData']:
        if isTree(tree['left']):
            return treeForeCast(tree['left'],inData,modelEval)
        else:
            return modelEval(tree['left'],inData)
    else: 
        if isTree(tree['right']):
            return treeForeCast(tree['right'],inData,modelEval)
        else:
            return modelEval(tree['right'],inData)

def createForeCast(tree,testData,modelEval=regTreeEval):
    m = len(testData)
    yHat = mat(zeros((m,1)))
    for i in range(m):
        yHat[i,0] = treeForeCast(tree,mat(testData[i]),modelEval)
    return yHat
```
### * 9.7 小结
&emsp;&emsp;优点：可以对复杂和非线性的数据建模
&emsp;&emsp;缺点：结果不易理解

# 3.无监督学习

# 4.其他

10.27 begin:,,,